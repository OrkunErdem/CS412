# -*- coding: utf-8 -*-
"""cs412.ipynb adlı not defterinin kopyası

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Zxlwq419SbCGA_ShopuaQ42tQCAOPBk3
"""

# Commented out IPython magic to ensure Python compatibility.
# load data
from google.colab import drive
drive.mount('/content/drive/')
# import the necessary libraries
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import re
from sklearn.model_selection import train_test_split
from pandas import DataFrame
import seaborn as sns
# %matplotlib inline

#RUN FOR PREDICT STEP 1
sampledf = pd.read_csv("/content/drive/My Drive/sample_submission.csv") 
testdf = pd.read_csv("/content/drive/My Drive/test.csv") 
traindf = pd.read_csv("/content/drive/My Drive/train.csv")

#DO NOT RUN FOR PREDICT THESE LINES FOR DATA ANALYSIS
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
cols = ['model','fuelType','transmission','brand',
       'mileage','mpg','engineSize','tax']

for c in cols:
    le.fit(list(traindf[c].values))
    traindf[c] = le.transform(list(traindf[c].values))

#DO NOT RUN FOR PREDICT THESE LINES FOR DATA ANALYSIS

plt.figure(figsize=(12, 12))
sns.heatmap(traindf.corr());

plt.figure(figsize=(12, 12))
heatmap = sns.heatmap(traindf.corr(), vmin=-1, vmax=1, annot=True)

traindf.max()

traindf.max()

#RUN FOR PREDICT STEP 2
traindf['age'] = 2021 - traindf['year']
testdf['age'] = 2021 - testdf['year']
traindf['mil']= 400000 -traindf['mileage']
testdf['mil']= 400000 -testdf['mileage']
traindf['fuel']= 500 -traindf['mpg']
testdf['fuel']= 500 -testdf['mpg']

#RUN FOR PREDICT STEP 3
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
cols = ['model','fuelType','transmission','brand',
       'mileage','mpg','engineSize','tax','mil','age','fuel']

for c in cols:
    le.fit(list(traindf[c].values))
    traindf[c] = le.transform(list(traindf[c].values))

plt.figure(figsize=(12, 12))
heatmap = sns.heatmap(traindf.corr(), vmin=-1, vmax=1, annot=True)

traindf.max()

#RUN FOR PREDICT STEP 4
X_train = traindf.drop(['price'], axis='columns')
y_train = traindf['price']
test=testdf

#RUN FOR PREDICT STEP 4
cols_for_none = ('model','brand','fuelType')
for c in cols_for_none:
    X_train[c] = X_train[c].fillna("None") 
    test[c] = test[c].fillna("None")

#RUN FOR PREDICT STEP 4
cols_for_zero = ('age','mileage','mpg','tax','tax(£)','mil','fuel')
for c in cols_for_zero:
    X_train[c] = X_train[c].fillna(0.0)
    test[c] = test[c].fillna(0.0)

#RUN FOR PREDICT STEP 4
cols_for_mode = ('engineSize','transmission')
for c in cols_for_mode:
    X_train[c] = X_train[c].fillna(X_train[c].mode())
    test[c] = test[c].fillna(test[c].mode())

traindf.mode()
# for analysis

#RUN FOR PREDICT STEP 5
#	ID	brand	  model	   year	    transmission	  mileage	  fuelType	mpg	  engineSize	tax	  tax(£)	price	  age	  mil	      fuel
# 0	  ford  	Fiesta	2019.0	  Manual	        18.0	    Petrol	  60.1	2.0	        145.0	145.0	  9995.0	2.0	  399982.0	439.9
X_train['engineSize'] = X_train['engineSize'].fillna(2.0)
X_train['transmission'] = X_train['transmission'].fillna('Manual')

test['engineSize'] = test['engineSize'].fillna(2.0)
test['transmission'] = test['transmission'].fillna('Manual')

#RUN FOR PREDICT STEP 6
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
cols = ['model','fuelType','transmission','brand',
       'mileage','mpg','engineSize','tax','mil','age','fuel']

for c in cols:
    le.fit(list(traindf[c].values))
    traindf[c] = le.transform(list(traindf[c].values))
for c in cols:
    le.fit(list(test[c].values))
    test[c] = le.transform(list(test[c].values))

#DO NOT RUN FOR PREDICT THESE LINES FOR DATA ANALYSIS

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
cols = ['model','fuelType','transmission','brand',
        'age','mileage','mpg','engineSize','tax']

for c in cols:
    le.fit(list(X_train[c].values))
    X_train[c] = le.transform(list(X_train[c].values))
    
for c in cols:
    le.fit(list(test[c].values))
    test[c] = le.transform(list(test[c].values))

#RUN FOR PREDICT STEP 7
X_train.drop('year', inplace=True, axis=1)
test.drop('year', inplace=True, axis=1)
X_train.drop('tax(£)', inplace=True, axis=1)
test.drop('tax(£)', inplace=True, axis=1)
X_train.drop('ID', inplace=True, axis=1)
test.drop('ID', inplace=True, axis=1)

X_train.info()
test.info()
# TO CHECK NULLS

# TO CHECK DATA
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_regression
#Show the important attributes in descending order
best_features = SelectKBest(score_func=f_regression, k=11)
top_features = best_features.fit(X_train,y_train)
scores = pd.DataFrame(top_features.scores_)
columns = pd.DataFrame(X_train.columns)
featureScores = pd.concat([columns, scores], axis=1)
featureScores.columns = ['Features','Scores']
print(featureScores.nlargest(11, 'Scores'))

#RUN FOR PREDICT STEP 8
X_train.drop('year', inplace=True, axis=1)
test.drop('year', inplace=True, axis=1)
X_train.drop('tax(£)', inplace=True, axis=1)
test.drop('tax(£)', inplace=True, axis=1)
X_train.drop('ID', inplace=True, axis=1)
test.drop('ID', inplace=True, axis=1)
X_train.drop('fuel', inplace=True, axis=1)
test.drop('fuel', inplace=True, axis=1)
X_train.drop('mil', inplace=True, axis=1)
test.drop('mil', inplace=True, axis=1)

X_train.info()
test.info()
#to check data

# TO DECIDE REGRESSION MODEL
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
import xgboost as xgb
from sklearn.model_selection import cross_val_score
lr = LinearRegression()
rr = RandomForestRegressor()
gbr = GradientBoostingRegressor()
xgb = xgb.XGBRegressor()
#Create function to displaying scores
def display_scores(scores):
    print("Scores: ", scores)
    print("Mean: ", scores.mean())
    print("Standard Deviation: ", scores.std())
#Training the Random Forest Regressor
print("Random Forest Regressor Scores")
scores = cross_val_score(rr, X_train, y_train, scoring='neg_mean_squared_error', cv=5)
random_forest_scores = np.sqrt(-scores)
display_scores(random_forest_scores)
print("\n")
#Training the Gradient Boosting Regressor
print('Gradient Boosting Regressor Scores')
scores = cross_val_score(gbr, X_train, y_train, scoring='neg_mean_squared_error', cv=5)
gradient_boosting_regressor = np.sqrt(-scores)
display_scores(gradient_boosting_regressor)
print("\n")
#Training the Linear Regression
print('Linear Regression Scores')
scores = cross_val_score(lr, X_train, y_train, scoring='neg_mean_squared_error', cv=5)
linear_regression = np.sqrt(-scores)
display_scores(linear_regression)
print("\n")
#Training the Extreme Gradient Boosting
print("xGB Scores")
scores = cross_val_score(xgb, X_train, y_train, scoring='neg_mean_squared_error', cv=5)
xgb_regressor = np.sqrt(-scores)
display_scores(xgb_regressor)

import requests, io 
from sklearn.metrics import mean_squared_error 
from sklearn.ensemble import RandomForestRegressor 
model = RandomForestRegressor(n_estimators = 50, random_state = 5)

model.fit(X_train, y_train)

y = model.predict(X_train)
rmse = float(format(np.sqrt(mean_squared_error(y_train, y)), '.3f'))
print("\nRMSE: ", rmse)
#TO IMPROVE MODEL

#RUN FOR PREDICT STEP 9
model = RandomForestRegressor(n_estimators = 50, random_state = 5)
model.fit(X_train, y_train)
result = model.predict(test)

print(result)

#RUN FOR PREDICT STEP 10
compare = pd.DataFrame()
compare['pri'] = result

#RUN FOR PREDICT STEP 11
compare.head()

compare.info()

#RUN FOR PREDICT STEP 12
compare = pd.DataFrame()
compare['price'] = result

#RUN FOR PREDICT STEP 13
compare.to_csv('sample4.csv')
from google.colab import files
files.download("sample4.csv")